input {
    # Spécifie que l'entrée de données est un fichier CSV local
    # "start_position" permet de lire le fichier depuis le début même s'il existe déjà
    # "sincedb_path" désactive le suivi d'état pour relire à chaque redémarrage
    file {
        path => "/usr/share/logstash/pollution_data.csv"
        start_position => "beginning"
        sincedb_path => "/dev/null"
    }
}

filter {
    # Le bloc 'csv' permet de transformer chaque ligne du fichier CSV en un document avec des champs nommés
    csv {
        separator => ","
        columns => ["date", "ville", "polluant", "valeur"]
    }
    mutate {
        # Convertit le champ "valeur" (au format texte) en nombre flottant
        convert => { "valeur" => "float" }
    }
    date {
        # Transforme le champ "date" (au format YYYY-MM-DD) en champ Elasticsearch "@timestamp"
        match => ["date", "YYYY-MM-dd"]
        target => "@timestamp"
    }
    mutate {
        # Ajoute dynamiquement un nom d’index basé sur la date du jour
        add_field => { "[@metadata][index_name]" => "%{+YYYY.MM.dd}" }
    }
}

output {
    # Envoie les données transformées vers Elasticsearch
    elasticsearch {
        hosts => ["http://elasticsearch:9200"]
        index => "pollution-%{[@metadata][index_name]}"
    }

    # Affiche le contenu JSON final dans la console pour vérifier les transformations
    stdout { codec => rubydebug }
}