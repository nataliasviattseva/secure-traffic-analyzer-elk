# Entrée : Logstash lit un fichier CSV contenant des logs de sécurité réseau
input {
  file {
    path => "/usr/share/logstash/network_security_logs.csv"
    start_position => "beginning"
    sincedb_path => "/dev/null"
  }
}

# Filtrage : transformation et enrichissement des données
filter {
  # Le bloc 'csv' transforme chaque ligne CSV en un document
  csv {
    separator => ","
    columns => [
      "timestamp", 
      "source_port", 
      "destination_port", 
      "protocol", 
      "action", 
      "bytes_sent", 
      "bytes_received", 
      "threat_level", 
      "latitude", 
      "longitude"
    ]
  }

  mutate {
    # Ajoute dynamiquement un nom d'index basé sur la date
    add_field => { "[@metadata][index]" => "iot-security-%{+YYYY.MM.dd}" }

    # Crée un champ geo_point au format [lon, lat]
    add_field => { "[location]" => "%{longitude},%{latitude}" }
  }

  # Conversion du champ "timestamp"
  date {
    match => ["timestamp", "ISO8601"]
    target => "@timestamp"
  }

  # Conversion de types
  mutate {
    convert => {
      "source_port"      => "integer"
      "destination_port" => "integer"
      "bytes_sent"       => "integer"
      "bytes_received"   => "integer"
      "latitude"         => "float"
      "longitude"        => "float"
    }
  }

  # Découpe du champ location en tableau [lon, lat] pour le type geo_point
  ruby {
    code => '
      if event.get("location")
        coords = event.get("location").split(",").map(&:to_f)
        event.set("location", coords)
      end
    '
  }
}

# Sortie : envoi vers Elasticsearch
output {
  elasticsearch {
    hosts => ["http://elasticsearch:9200"]
    index => "%{[@metadata][index]}"
  }

  stdout {
    codec => rubydebug
  }
}
